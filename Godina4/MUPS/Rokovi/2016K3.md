**1. [15] Navesti tipične načine deljenja podataka sa aspekta broja invalidacija. Objasniti kako zaključci analize deljenja utiču na organizaciju i veličinu kataloga.**

- Kod i _read-only_ objekti se nikad ne invalidiraju
- _Mostly read_ objekti se retko invalidiraju
- Objekti u koje se često upisuje se često invalidiraju, ali baš zbog toga ne postoji veliki broj njihovih kopija

U većini sistema, ovo utiče na to da katalog ne mora da bude organizovan kao _full-map_, nego može da se smanji njegova veličina bez ikakvih posledica po performanse

**2. [15] Objasniti tehniku oranizacije kataloga sa dinamičkim poinerima (_Dir<sub>i</sub> DP_), kao i osnovne akcije.**

Svaki ulaz u katalog po M dimenziji sadrži pokazivač na kružnu listu pointera. Svaki od tih pointera pokazuje na jedan ulaz po P dimenziji, tj. na _deskriptor_ jednog procesora. U tom deskriptoru se nalazi samo informacija da li je taj procesor menjao blok (_dirty_ bit), a samo postojanje pointera na deskriptor ukazuje na to da taj procesor sadrži taj blok (_valid_ bit nije potreban).

Osnovne akcije:
- RM: Dohvata se jedan slobodan pointer na deskriptor. Ukoliko slobodan ne postoji, neki blok nekog procesora se invalidira, i jedan pointer onda postaje slobodan. Taj pointer se ubacuje u kružnu listu bloka za koji je generisan read miss. Ukoliko se u listi nalazio deskriptor sa setovanim _dirty_ bitom, od tog procesora se traži flush, i _dirty_ bit se resetuje. 
- WM: Šalje se invalidaciona poruka svim procesorima u kružnoj listi za trenutni blok, i svi se izbacuju iz liste. Dohvata se jedan slobodan pointer na deskriptor. Ukoliko ne postoji, radi se isto što i kod RM. Dobijeni pointer se ubacuje u sada praznu kružnu listu, i setuje mu se _dirty_ bit.
- WH: Ako pre toga nije bilo upisa u taj blok, šalje se poruka matičnom čvoru, i matični čvor izbaci sve ostale pointere iz liste, i invalidira ih.

**3. [15]. Nacrtati organizaciju hijerarhijskog sistema sa dva nivoa zasnovanog na magistralama i sa globalnom memorijom. Ako se održava inkluzija, opisati akcije koje se odvijaju između dva nivoa u oba smera.**

Na procesoru postoje L1 keševi, koji su magistralom vezani na svoj L2 keš, a L2 keševi su vezani na globalnu memoriju.

U slučaju da se koristi WT politika, nije potrebna sledeća akcija, a u slučaju da se koristi WB politika, potrebna je: Kada L2 keš snoopuje zahtev za nekim podatkom koji se nalazi u L1 kešu, i dirty je (u L2 je ovo stanje _dirty-invalid_), javi L1 kešu da treba da _flush_-uje taj blok, kako bi ga L2 keš poslao na magistralu.

U slučaju da se desi RH ili WH u L1 kešu, i u slučaju da je LRU algoritam zamene, podatak o tome da se desio _hit_ se mora propagirati do L2 keša, kako bi taj podatak bio najskorije korišćen i u tom kešu.

**4. [15] Precizno definisati parametre direktnih interkonekcionih mreža. Za svaki od njih objasnosti zbog čega je važan i koje su poželjne vrednosti.**

- _d_ - stepen čvora - poželjno je da bude što veći, jer je onda veća šansa da je čvor kome želimo da pošaljemo poruku sused
- _D_ - Prečnik grafa - poželjno je da bude što manji, jer tako minimizujemo najgori slučaj broja hopova poslate poruke
- Simetričnost grafa - poželjno je da bude simetričan, jer onda svi čvorovi vide isti graf, i u slučaju uniformnih poruka, iz svih čvorova imamo isto opterećenje
- Regularnost grafa - graf je regularan ako je stepen svih čvorova isti. Poželjno je da bude regularan, jer onda uprošćava komunikaciju
- _B_ - Propusni opseg najmanje bisekcije - bisekcija je podela grafa na dva dela, najmanja bisekcija je bisekcija koja prolazi kroz najmanje grana. Propusni opseg je bitan za _all-to-all_ algoritme, i treba nam što veći, da bi bilo što manje kašnjenje.

**5. [15] Zadati deo nekog CUDA jezgra poseduje jedan značajan nedostatak. Koji je to nedostatak, kako utiče na izvršavanje koda i kako bi se mogao prevazići?**

```C
for (int i = 0; i < blockDim.x; i++) {
    if (threadIdx.x % 2) sdata[threadIdx.x] ^= sdata[i] & MASK1;
    else sdata[threadIdx.x] ^= sdata[i] & MASK2;
}
```

**Rešenje:**

Nedostatak je u tome što postoji branch divergence. Problem može da se prevaziđe tako što se pokrene duplo manje niti, koje rade duplo veći posao - svaka nova nit radi i ono što radi stara parna nit, i ono što radi stara neparna nit

```C
for (int i = 0; i < blockDim.x; i++) {
    sdata[threadIdx.x * 2] ^= sdata[i] & MASK1;
    sdata[threadIdx.x * 2 + 1] ^= sdata[i] & MASK2;
}
```

**6. [10] Kako se vrši alokacija registara na grafičkom procesoru i da li to može da utiče na performanse izvršavanog jezgra?**

Svaka multiprocesorska jedinica ima N registara, i registri se dinamički dodeljuju nitima blokova koji se trenutno izvršavaju na konkretnoj multiprocesorskoj jedinici. Broj blokova koji mogu da se izvršavaju na jednoj jedinici direktno zavisi od broja raspoloživih registara te jedinice - izvršavaće se onoliko registara koliko je moguće, a da svi blokovi imaju onoliko registara kolike su njihove potrebe. Ako bi dodavanje bloka na multiprocesorsku jedinicu prouzrokovalo nedostatak registara, taj blok neće biti izvršavan na toj jedinici.

**7. [15] Zadato jezgro CUDA vrši kompletnu obradu korišćenjem globalne memorije. Napisati ponovo jezgro, tako da se koristi deljena memorija. Obrazložiti kako korišćenje deljene memorije može da doprinese poboljšanju performansi u ovom konkretnom slučaju.**

```C
__global__ void averageKernel (int* devA, int* devB, int n){
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    int left, right;
    left = idx > 0 ? idx - 1 : 0;
    right = idx < n - 1 ? idx + 1 : n - 1;
    if( idx < n ) {
        devB[idx] = (devA[left] + devA[idx] + devA[right]) / 3;
    }
}
```

**Rešenje:**

Problem je ovde u tome što svaka nit čita tri elementa iz globalne memorije, što znači da inače imamo tri puta više čitanja nego što nam treba. Optimizacija je da sve niti jednom učitaju sve elemente u deljenu memoriju, a onda da tri puta čitaju iz deljene, koja je brža od globalne.

```C
__global__ void averageKernel (int* devA, int* devB, int n){
    __shared__ int sharedDevA[n];
    int idx = blockDim.x * blockIdx.x + threadIdx.x;
    int left, right;

    sharedDevA[idx] = devA[idx];
    __syncthreads();

    left = idx > 0 ? idx - 1 : 0;
    right = idx < n - 1 ? idx + 1 : n - 1;
    if( idx < n ) {
        devB[idx] = (sharedDevA[left] + sharedDevA[idx] + sharedDevA[right]) / 3;
    }
}
```
